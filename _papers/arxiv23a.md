---
layout: paper
permalink: /papers/arxiv23a/
title: "On the Proactive Generation of Unsafe Images From Text-To-Image Models Using Benign Prompts"
teaser: "/assets/img/publication_preview/arxiv23a.png"
teaser_caption: "Comparison of generated images of three SD versions and real images. The prompt describes an artistic style, i.e., by miles johnston. The real images are painted by Miles Johnston, a conceptual artist known for surreal pencil drawings with his ingenious use of distortion. We observe that SD-1.5 directly generates nude female figures. Images generated by SD-2.1 include nude female figures and disturbing close-ups of faces. SDXL successfully suppresses unsafe content in the generated pencil d...
disclaimer: This website contains unsafe images that might be offensive.

authors: 
  - name: "Yixin Wu"
    affiliation: "CISPA Helmholtz Center for Information Security"
  - name: "Ning Yu"
    affiliation: "Salesforce Research"
  - name: "Michael Backes"
    affiliation: "CISPA Helmholtz Center for Information Security"
  - name: "Yun Shen"
    affiliation: "NetApp"
  - name: "Yang Zhang"
    affiliation: "CISPA Helmholtz Center for Information Security"
conference: "arXiv"
abstract: "Text-to-image models like Stable Diffusion have had a profound impact on daily life by enabling the generation of photorealistic images from textual prompts, fostering creativity, and enhancing visual experiences across various applications. However, these models also pose risks. Previous studies have successfully demonstrated that manipulated prompts can elicit text-to-image models to generate unsafe images, e.g., hateful meme variants. Yet, these studies only unleash the harmful power of text-to-image models in a passive manner. In this work, we focus on the proactive generation of unsafe images using targeted benign prompts via poisoning attacks. We propose two poisoning attacks: a basic attack and a utility-preserving attack. We qualitatively and quantitatively evaluate the proposed attacks using four representative hateful memes and multiple query prompts. Experimental results indicate that text-to-image models are vulnerable to the basic attack even with five poisoning samples. However, the poisoning effect can inadvertently spread to non-targeted prompts, leading to undesirable side effects. Root cause analysis identifies conceptual similarity as an important contributing factor to the side effects. To address this, we introduce the utility-preserving attack as a viable mitigation strategy to maintain the attack stealthiness, while ensuring decent attack performance. Our findings underscore the potential risks of adopting text-to-image models in real-world scenarios, calling for future research and safety measures in this space."
paper: "https://arxiv.org/pdf/2310.16613.pdf"
code: "."
citation: |
  @article{WYBSZ23,
    title={On the Proactive Generation of Unsafe Images From Text-To-Image Models Using Benign Prompts},
    author={Wu, Yixin and Yu, Ning and Backes, Michael and Shen, Yun and Zhang, Yang},
    journal={arXiv},
    year={2023}
  }
---

{% include paper_page.liquid %}